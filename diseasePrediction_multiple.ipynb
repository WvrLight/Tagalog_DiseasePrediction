{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras_crf import CRFModel\n",
    "\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns  # for statistical data visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "folder_name = 'cfg'\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"word_list\")) as json_file:\n",
    "    wordIdList = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"ner_config\")) as json_file:\n",
    "    ner_config = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"tags\")) as json_file:\n",
    "    tags = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"symptom_list\")) as json_file:\n",
    "    symptom_list = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"disease_list\")) as json_file:\n",
    "    disease_list = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/sentences_by_respondent.csv'\n",
    "\n",
    "dataset = pd.read_csv(data_path)\n",
    "dataset = dataset.dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['respondent_idx', 'sentence', 'result'], dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    tubercolosis\n",
       "17       pneumonia\n",
       "3        pneumonia\n",
       "13             flu\n",
       "19        diarrhea\n",
       "16          dengue\n",
       "10          dengue\n",
       "Name: result, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Put each sentence in their own group based on respondent ID\n",
    "x = dataset.groupby(['respondent_idx', 'result']).sentence.apply(list).reset_index()\n",
    "y = x['result']\n",
    "x = x.drop('result', axis=1)\n",
    "\n",
    "respondent_groups_train, respondent_groups, respondent_result_train, respondent_result = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=1)\n",
    "\n",
    "respondent_list = []\n",
    "for val in respondent_groups.values:\n",
    "    respondent_list.append(val[1])\n",
    "\n",
    "respondent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TglStemmer import stemmer\n",
    "\n",
    "def preprocess_data():\n",
    "    preprocessed_respondent_list = []\n",
    "    for respondent in respondent_list:\n",
    "        preprocessed_respondent = []\n",
    "        for sentence in respondent:\n",
    "            preprocessed_respondent.append(stemmer('2', sentence, '1'))\n",
    "        preprocessed_respondent_list.append(preprocessed_respondent)\n",
    "    return preprocessed_respondent_list\n",
    "\n",
    "#respondent_list = preprocess_data()\n",
    "#respondent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"{}/{}.json\".format('cfg', \"stopwords-tl\")) as json_file:\n",
    "    stopwords = json.load(json_file)\n",
    "\n",
    "def remove_stopwords(tokenizedSentence):\n",
    "    for stopword in stopwords:\n",
    "        for word in tokenizedSentence:\n",
    "            if (word == stopword):\n",
    "                print(\"a\")\n",
    "                #tokenizedSentence.remove(word)\n",
    "    return tokenizedSentence\n",
    "\n",
    "# cleaned_respondent_list = []\n",
    "# for respondent in respondent_list:\n",
    "#     cleaned_respondent = []\n",
    "#     for sentence in respondent:\n",
    "#         cleaned_respondent.append(remove_stopwords(sentence))\n",
    "#     cleaned_respondent_list.append(cleaned_respondent)\n",
    "\n",
    "#respondent_list = cleaned_respondent_list\n",
    "#respondent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nitong', 'raan', 'ramdam', 'pagod', 'sikip', 'dibdib']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each sentence in all groups\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = []\n",
    "for respondent in respondent_list:\n",
    "    sentence_group = []\n",
    "    for sentence in respondent:\n",
    "        sentence_group.append(word_tokenize(sentence))\n",
    "    tokenized_sentences.append(sentence_group)\n",
    "    \n",
    "tokenized_sentences[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_IDX = ner_config[\"n_words\"] - 2\n",
    "UNK_IDX = ner_config[\"n_words\"] - 1\n",
    "\n",
    "def convert_sentence_to_idx(tokenizedSentence):\n",
    "    sentence2idx = []\n",
    "    for word in tokenizedSentence:\n",
    "        wordFound = False\n",
    "        for key, val in wordIdList.items():\n",
    "            if (word == key):\n",
    "                wordFound = True\n",
    "                sentence2idx.append(val)\n",
    "        if (not wordFound):\n",
    "            sentence2idx.append(UNK_IDX)\n",
    "    while (len((sentence2idx)) < ner_config[\"maxlen\"]):\n",
    "        sentence2idx.append(END_IDX)\n",
    "    return sentence2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[16, 0, 39, 42, 19, 49, 75, 75], [28, 62, 1, 34, 59, 75, 75, 75]],\n",
       " [[59, 62, 48, 38, 75, 75, 75, 75], [20, 6, 44, 37, 39, 50, 15, 13]],\n",
       " [[59, 62, 57, 75, 75, 75, 75, 75], [47, 6, 50, 15, 13, 75, 75, 75]],\n",
       " [[24, 39, 20, 26, 55, 75, 75, 75], [26, 22, 6, 25, 29, 75, 75, 75]],\n",
       " [[2, 71, 26, 66, 75, 75, 75, 75], [39, 44, 46, 24, 75, 75, 75, 75]],\n",
       " [[47, 3, 33, 75, 75, 75, 75, 75], [52, 57, 34, 23, 75, 75, 75, 75]],\n",
       " [[76, 57, 60, 26, 22, 75, 75, 75],\n",
       "  [53, 56, 41, 33, 75, 75, 75, 75],\n",
       "  [25, 44, 38, 75, 75, 75, 75, 75]]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each tokenized sentence to its word id counterparts\n",
    "idx_sentences = []\n",
    "for respondent in tokenized_sentences:\n",
    "    sentence_group = []\n",
    "    for sentence in respondent:\n",
    "        sentence_group.append(convert_sentence_to_idx(sentence))\n",
    "    idx_sentences.append(sentence_group)\n",
    "idx_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Embedding, Bidirectional\n",
    "from tf2crf import CRF, ModelWithCRFLoss\n",
    "\n",
    "def initialize_model(model_type):\n",
    "    inputs = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
    "    output = Embedding(ner_config['n_words'], ner_config['word_embedding_size'],\n",
    "                    trainable=True, mask_zero=True)(inputs)\n",
    "    if (model_type == 'LSTM'):\n",
    "        bi_rnn = Bidirectional(LSTM(units=ner_config['word_embedding_size'],\n",
    "                                    return_sequences=True,\n",
    "                                    dropout=0.5,\n",
    "                                    recurrent_dropout=0.5,\n",
    "                                    kernel_initializer=tf.keras.initializers.he_normal()))(output)\n",
    "        rnn = LSTM(units=ner_config['word_embedding_size'] * 2,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.5,\n",
    "                    recurrent_dropout=0.5,\n",
    "                    kernel_initializer=tf.keras.initializers.he_normal())(bi_rnn)\n",
    "    else:\n",
    "        bi_rnn = Bidirectional(GRU(units=ner_config['word_embedding_size'],\n",
    "                                    return_sequences=True,\n",
    "                                    dropout=0.5,\n",
    "                                    recurrent_dropout=0.5,\n",
    "                                    kernel_initializer=tf.keras.initializers.he_normal()))(output)\n",
    "        rnn = GRU(units=ner_config['word_embedding_size'] * 2,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.5,\n",
    "                    recurrent_dropout=0.5,\n",
    "                    kernel_initializer=tf.keras.initializers.he_normal())(bi_rnn)\n",
    "    crf = CRF(units=ner_config['n_tags'], dtype='float32')\n",
    "    output = crf(rnn)\n",
    "    base_model = Model(inputs, output)\n",
    "    ner_model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
    "    ner_model.build(ner_config['shape'])\n",
    "\n",
    "    if (model_type == 'LSTM'):\n",
    "        ner_model.load_weights(\"bilstm\")\n",
    "    else:\n",
    "        ner_model.load_weights(\"bigru\")\n",
    "    return ner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_with_crf_loss_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_2 (Functional)        ((None, None),            3272112   \n",
      "                              (None, None, 3),                   \n",
      "                              (None,),                           \n",
      "                              (3, 3))                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,272,116\n",
      "Trainable params: 3,272,112\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ner_model = tf.keras.models.load_model('bigru.h5')\n",
    "ner_model = initialize_model('GRU')\n",
    "ner_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_sentences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_INDEX = 0\n",
    "\n",
    "# For a given sentence, predict the words which are related to symptom information\n",
    "def recognize_symptoms_in_sentence(sentence2idx, tokenizedSentence):\n",
    "    p = ner_model.predict(np.array([sentence2idx]))\n",
    "    #p = np.argmax(p, axis=-1)\n",
    "\n",
    "    input_symptom_list = []\n",
    "\n",
    "    # Iterate through the entire sentence\n",
    "    \n",
    "    for idx, (w, pred) in enumerate(zip(sentence2idx, p[0])):\n",
    "        if (tags[pred] == 'B-SYMPTOM'):\n",
    "            symptom_word = tokenizedSentence[idx]\n",
    "            \n",
    "            # Check for additional words for a symptom\n",
    "            temp_idx = idx + 1\n",
    "            if (temp_idx < (len(tokenizedSentence) - 1) and p[0][temp_idx] == I_INDEX):\n",
    "                while (p[0][temp_idx] == I_INDEX):\n",
    "                    symptom_word = symptom_word + \" \" + tokenizedSentence[temp_idx]\n",
    "                    if (temp_idx != len(tokenizedSentence) - 1): \n",
    "                        temp_idx += 1\n",
    "                    else:\n",
    "                        break\n",
    "            input_symptom_list.append(symptom_word)\n",
    "        if (idx == len(tokenizedSentence) - 1):\n",
    "            break\n",
    "    return input_symptom_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "STRING_MATCH_PERCENTAGE = 0.75\n",
    "\n",
    "# Match each symptom with the trained symptom list\n",
    "def symptoms_to_boolean(input_symptom_list):\n",
    "    input_to_boolean = []\n",
    "\n",
    "    for symptom in symptom_list:\n",
    "        symptomInList = False\n",
    "        for input_symptom in input_symptom_list:\n",
    "            if (symptom in input_symptom or SequenceMatcher(None, input_symptom, symptom).ratio() >= STRING_MATCH_PERCENTAGE):\n",
    "                symptomInList = True\n",
    "        if (symptomInList):\n",
    "            input_to_boolean.append(1)\n",
    "        else:\n",
    "            input_to_boolean.append(0)\n",
    "    return input_to_boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each tokenized sentence to its word id counterparts\n",
    "boolean_symptoms = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for respondent in idx_sentences:\n",
    "    j = 0\n",
    "    symptom_group = []\n",
    "    for sentence in respondent:\n",
    "        symptom_group.append(symptoms_to_boolean(recognize_symptoms_in_sentence(sentence, tokenized_sentences[i][j])))\n",
    "        j += 1\n",
    "    boolean_symptoms.append(symptom_group)\n",
    "    i += 1\n",
    "boolean_symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each group, merge all existing symptoms into one array\n",
    "def merge_symptom_info(symptom_arrays):\n",
    "    # Initialize null array\n",
    "    merged_symptoms = []\n",
    "    for symptom in symptom_list:\n",
    "        merged_symptoms.append(0)\n",
    "\n",
    "    for symptom_group in symptom_arrays:\n",
    "        for i, symptom in enumerate(symptom_group):\n",
    "            if symptom == 1:\n",
    "                merged_symptoms[i] = 1\n",
    "    return merged_symptoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge symptoms per respondent\n",
    "respondent_symptoms = []\n",
    "for respondent in boolean_symptoms:\n",
    "    respondent_symptoms.append(merge_symptom_info(respondent))\n",
    "respondent_symptoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([('lagnat', 'sakit ulo', 'suka', 'hilo', 'ubo', 'hina', ...)],\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "symptom_columns = []\n",
    "for symptom in symptom_list:\n",
    "    symptom_columns.append([symptom,],)\n",
    "\n",
    "cols = pd.MultiIndex.from_arrays(symptom_columns)\n",
    "print(cols)\n",
    "\n",
    "input_frame = pd.DataFrame(respondent_symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('naiveBayes.pkl', 'rb') as f:\n",
    "    naiveBayes = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tubercolosis', 'pneumonia', 'pneumonia', 'flu', 'diarrhea',\n",
       "       'dengue', 'dengue'], dtype='<U12')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_frame = scaler.fit_transform(input_frame)\n",
    "\n",
    "prediction_results = naiveBayes.predict(input_frame)\n",
    "prediction_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED: Convert diseases to respective IDs for comparison\n",
    "def diseases_to_idx(list_of_disease_strings):\n",
    "    list_of_disease_idx = []\n",
    "    for disease in list_of_disease_strings:\n",
    "        for key, val in disease_list.items():\n",
    "            if (disease == key):\n",
    "                list_of_disease_idx.append(val)\n",
    "    return list_of_disease_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20    tubercolosis\n",
      "17       pneumonia\n",
      "3        pneumonia\n",
      "13             flu\n",
      "19        diarrhea\n",
      "16          dengue\n",
      "10          dengue\n",
      "Name: result, dtype: object\n",
      "['tubercolosis' 'pneumonia' 'pneumonia' 'flu' 'diarrhea' 'dengue' 'dengue']\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n",
      "[[[5 0]\n",
      "  [0 2]]\n",
      "\n",
      " [[6 0]\n",
      "  [0 1]]\n",
      "\n",
      " [[6 0]\n",
      "  [0 1]]\n",
      "\n",
      " [[5 0]\n",
      "  [0 2]]\n",
      "\n",
      " [[6 0]\n",
      "  [0 1]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix\n",
    "\n",
    "true_results = respondent_result\n",
    "print(true_results)\n",
    "print(prediction_results)\n",
    "#true_results = true_results.drop_duplicates()\n",
    "# true_results = true_results['result']\n",
    "\n",
    "#y_pred = diseases_to_idx(prediction_results)\n",
    "#y_true = diseases_to_idx(true_results.values.tolist())\n",
    "\n",
    "print(\"Precision:\", precision_score(true_results,\n",
    "      prediction_results, average='macro', zero_division=0))\n",
    "print(\"Recall:\", recall_score(true_results, prediction_results, average='macro', zero_division=0))\n",
    "print(\"F1-score:\", f1_score(true_results,\n",
    "      prediction_results, average='macro', zero_division=0))\n",
    "\n",
    "print(multilabel_confusion_matrix(true_results, prediction_results))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
