{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras_crf import CRFModel\n",
    "\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns  # for statistical data visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "folder_name = 'cfg'\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"word_list\")) as json_file:\n",
    "    wordIdList = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"ner_config\")) as json_file:\n",
    "    ner_config = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"tags\")) as json_file:\n",
    "    tags = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"symptom_list\")) as json_file:\n",
    "    symptom_list = json.load(json_file)\n",
    "\n",
    "with open(\"{}/{}.json\".format(folder_name, \"disease_list\")) as json_file:\n",
    "    disease_list = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/sentences_by_respondent.csv'\n",
    "\n",
    "data = pd.read_csv(data)\n",
    "#data = data.dropna(axis=0)\n",
    "\n",
    "# Creating a dataframe with 75%\n",
    "# values of original dataframe\n",
    "df_train = data.sample(frac=0.67)\n",
    "\n",
    "# Creating dataframe with\n",
    "# rest of the 25% values\n",
    "df_test = data.drop(df_train.index)\n",
    "\n",
    "dataset = df_test.drop(['result'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['respondent_idx', 'sentence'], dtype='object')"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ako ay nakakaranas ng hirap sa paghinga pagkatapos tumakbo o kaya pagtapos ng mahabang lakaran.'],\n",
       " ['Nilalamig dahil sa lagnat at masakit ang katawan.',\n",
       "  'Masakit ang ulo, nagsusuka at madalas nahihilo.'],\n",
       " ['Mataas ang lagnat, inuubong may plema, nahihilo, at nagsusuka.'],\n",
       " ['Nahihirapan din sa pagtulog.'],\n",
       " ['Masakit ang dibdib at mabilis ang pagtakbo ng puso.'],\n",
       " ['Madalas ang pagdumi ng basa o parang tubig.'],\n",
       " ['Laging inuubo at naninikip ang dibdib ko.'],\n",
       " ['Nagpapawis tuwing gabi tapos nawalan din ng gana kumain.'],\n",
       " ['Masakit ang batok at nahihilo.'],\n",
       " ['Pananakit ng ulo at nilalagnat.',\n",
       "  'Masakit na pag-nguya at pamamaga ng gilagid.'],\n",
       " ['Mataas na lagnat, pamamantal, at madaling pag-papasa.',\n",
       "  'Malubhang pananakit ng ulo, pananakit ng kalamnan, at pananakit ng kasu-kasuan.'],\n",
       " ['Matamlay, nanghihina, at madalas umihi.',\n",
       "  'Matagal na pag-galing ng sugat at nabawasn ang timbang.'],\n",
       " ['Pumipintig ang ulo at matinding pananakit ng ulo.'],\n",
       " ['Madalas din po ang pagdumi ko tapos nabawasan ang timbang.'],\n",
       " ['Mula nung sabado nag-umpisa yung pag-ubo ko saka hirap sa paghinga.']]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put each sentence in their own group based on respondent ID\n",
    "respondent_groups = dataset.groupby(['respondent_idx']).sentence.apply(list).reset_index()\n",
    "\n",
    "respondent_list = []\n",
    "for val in respondent_groups.values:\n",
    "    respondent_list.append(val[1])\n",
    "\n",
    "respondent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TglStemmer import stemmer\n",
    "\n",
    "def preprocess_data():\n",
    "    preprocessed_respondent_list = []\n",
    "    for respondent in respondent_list:\n",
    "        preprocessed_respondent = []\n",
    "        for sentence in respondent:\n",
    "            preprocessed_respondent.append(stemmer('2', sentence, '1'))\n",
    "        preprocessed_respondent_list.append(preprocessed_respondent)\n",
    "    return preprocessed_respondent_list\n",
    "\n",
    "#respondent_list = preprocess_data()\n",
    "#respondent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"{}/{}.json\".format('cfg', \"stopwords-tl\")) as json_file:\n",
    "    stopwords = json.load(json_file)\n",
    "\n",
    "def remove_stopwords(tokenizedSentence):\n",
    "    for stopword in stopwords:\n",
    "        for word in tokenizedSentence:\n",
    "            if (word == stopword):\n",
    "                print(\"a\")\n",
    "                #tokenizedSentence.remove(word)\n",
    "    return tokenizedSentence\n",
    "\n",
    "# cleaned_respondent_list = []\n",
    "# for respondent in respondent_list:\n",
    "#     cleaned_respondent = []\n",
    "#     for sentence in respondent:\n",
    "#         cleaned_respondent.append(remove_stopwords(sentence))\n",
    "#     cleaned_respondent_list.append(cleaned_respondent)\n",
    "\n",
    "#respondent_list = cleaned_respondent_list\n",
    "#respondent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ako',\n",
       " 'ay',\n",
       " 'nakakaranas',\n",
       " 'ng',\n",
       " 'hirap',\n",
       " 'sa',\n",
       " 'paghinga',\n",
       " 'pagkatapos',\n",
       " 'tumakbo',\n",
       " 'o',\n",
       " 'kaya',\n",
       " 'pagtapos',\n",
       " 'ng',\n",
       " 'mahabang',\n",
       " 'lakaran',\n",
       " '.']"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each sentence in all groups\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = []\n",
    "for respondent in respondent_list:\n",
    "    sentence_group = []\n",
    "    for sentence in respondent:\n",
    "        sentence_group.append(word_tokenize(sentence))\n",
    "    tokenized_sentences.append(sentence_group)\n",
    "    \n",
    "tokenized_sentences[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_IDX = ner_config[\"n_words\"] - 2\n",
    "UNK_IDX = ner_config[\"n_words\"] - 1\n",
    "\n",
    "def convert_sentence_to_idx(tokenizedSentence):\n",
    "    sentence2idx = []\n",
    "    for word in tokenizedSentence:\n",
    "        wordFound = False\n",
    "        for key, val in wordIdList.items():\n",
    "            if (word == key):\n",
    "                wordFound = True\n",
    "                sentence2idx.append(val)\n",
    "        if (not wordFound):\n",
    "            sentence2idx.append(UNK_IDX)\n",
    "    while (len((sentence2idx)) < ner_config[\"maxlen\"]):\n",
    "        sentence2idx.append(END_IDX)\n",
    "    return sentence2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[31,\n",
       "   2,\n",
       "   125,\n",
       "   64,\n",
       "   19,\n",
       "   52,\n",
       "   117,\n",
       "   109,\n",
       "   20,\n",
       "   177,\n",
       "   45,\n",
       "   136,\n",
       "   64,\n",
       "   83,\n",
       "   61,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[92,\n",
       "   176,\n",
       "   52,\n",
       "   164,\n",
       "   124,\n",
       "   54,\n",
       "   57,\n",
       "   127,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188],\n",
       "  [79,\n",
       "   57,\n",
       "   4,\n",
       "   101,\n",
       "   85,\n",
       "   124,\n",
       "   144,\n",
       "   168,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[131,\n",
       "   57,\n",
       "   164,\n",
       "   101,\n",
       "   60,\n",
       "   55,\n",
       "   49,\n",
       "   101,\n",
       "   168,\n",
       "   101,\n",
       "   124,\n",
       "   85,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[143,\n",
       "   154,\n",
       "   52,\n",
       "   104,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[79,\n",
       "   57,\n",
       "   81,\n",
       "   124,\n",
       "   93,\n",
       "   57,\n",
       "   172,\n",
       "   64,\n",
       "   14,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[69,\n",
       "   57,\n",
       "   56,\n",
       "   64,\n",
       "   116,\n",
       "   177,\n",
       "   119,\n",
       "   87,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[63,\n",
       "   120,\n",
       "   124,\n",
       "   36,\n",
       "   57,\n",
       "   81,\n",
       "   44,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[123,\n",
       "   122,\n",
       "   58,\n",
       "   183,\n",
       "   185,\n",
       "   154,\n",
       "   64,\n",
       "   147,\n",
       "   18,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[79,\n",
       "   57,\n",
       "   48,\n",
       "   124,\n",
       "   168,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[129,\n",
       "   64,\n",
       "   4,\n",
       "   124,\n",
       "   112,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188],\n",
       "  [79,\n",
       "   171,\n",
       "   25,\n",
       "   124,\n",
       "   175,\n",
       "   64,\n",
       "   29,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[131,\n",
       "   171,\n",
       "   164,\n",
       "   101,\n",
       "   103,\n",
       "   101,\n",
       "   124,\n",
       "   90,\n",
       "   152,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188],\n",
       "  [140,\n",
       "   170,\n",
       "   64,\n",
       "   4,\n",
       "   101,\n",
       "   170,\n",
       "   64,\n",
       "   189,\n",
       "   101,\n",
       "   124,\n",
       "   170,\n",
       "   64,\n",
       "   138,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[30,\n",
       "   101,\n",
       "   184,\n",
       "   101,\n",
       "   124,\n",
       "   144,\n",
       "   135,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188],\n",
       "  [94,\n",
       "   171,\n",
       "   9,\n",
       "   64,\n",
       "   32,\n",
       "   124,\n",
       "   189,\n",
       "   57,\n",
       "   169,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[80,\n",
       "   57,\n",
       "   4,\n",
       "   124,\n",
       "   50,\n",
       "   170,\n",
       "   64,\n",
       "   4,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[69,\n",
       "   154,\n",
       "   67,\n",
       "   57,\n",
       "   56,\n",
       "   44,\n",
       "   183,\n",
       "   157,\n",
       "   57,\n",
       "   169,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]],\n",
       " [[70,\n",
       "   13,\n",
       "   134,\n",
       "   133,\n",
       "   75,\n",
       "   10,\n",
       "   44,\n",
       "   142,\n",
       "   19,\n",
       "   52,\n",
       "   117,\n",
       "   34,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188]]]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each tokenized sentence to its word id counterparts\n",
    "idx_sentences = []\n",
    "for respondent in tokenized_sentences:\n",
    "    sentence_group = []\n",
    "    for sentence in respondent:\n",
    "        sentence_group.append(convert_sentence_to_idx(sentence))\n",
    "    idx_sentences.append(sentence_group)\n",
    "idx_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 19, 300)           57000     \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 19, 600)          1442400   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 19, 600)           2882400   \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 19, 4)            2404      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 19, 4)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,384,204\n",
      "Trainable params: 4,384,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ner_model = tf.keras.models.load_model('bilstm.h5')\n",
    "ner_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_sentences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_INDEX = 0\n",
    "\n",
    "# For a given sentence, predict the words which are related to symptom information\n",
    "def recognize_symptoms_in_sentence(sentence2idx, tokenizedSentence):\n",
    "    p = ner_model.predict(np.array([sentence2idx]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "\n",
    "    input_symptom_list = []\n",
    "\n",
    "    # Iterate through the entire sentence\n",
    "    for idx, (w, pred) in enumerate(zip(sentence2idx, p[0])):\n",
    "        if (tags[pred] == 'B-SYMPTOM'):\n",
    "            symptom_word = tokenizedSentence[idx]\n",
    "            \n",
    "            # Check for additional words for a symptom\n",
    "            temp_idx = idx + 1\n",
    "            if (temp_idx < (len(tokenizedSentence) - 1) and p[0][temp_idx] == I_INDEX):\n",
    "                while (p[0][temp_idx] == I_INDEX):\n",
    "                    symptom_word = symptom_word + \" \" + tokenizedSentence[temp_idx]\n",
    "                    if (temp_idx != len(tokenizedSentence) - 1): \n",
    "                        temp_idx += 1\n",
    "                    else:\n",
    "                        break\n",
    "            input_symptom_list.append(symptom_word)\n",
    "        if (idx == len(tokenizedSentence) - 1):\n",
    "            break\n",
    "    return input_symptom_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "STRING_MATCH_PERCENTAGE = 0.75\n",
    "\n",
    "# Match each symptom with the trained symptom list\n",
    "def symptoms_to_boolean(input_symptom_list):\n",
    "    input_to_boolean = []\n",
    "\n",
    "    for symptom in symptom_list:\n",
    "        symptomInList = False\n",
    "        for input_symptom in input_symptom_list:\n",
    "            if (input_symptom in symptom or SequenceMatcher(None, input_symptom, symptom).ratio() >= STRING_MATCH_PERCENTAGE):\n",
    "                symptomInList = True\n",
    "        if (symptomInList):\n",
    "            input_to_boolean.append(1)\n",
    "        else:\n",
    "            input_to_boolean.append(0)\n",
    "    return input_to_boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 799ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [[1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each tokenized sentence to its word id counterparts\n",
    "boolean_symptoms = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for respondent in idx_sentences:\n",
    "    j = 0\n",
    "    symptom_group = []\n",
    "    for sentence in respondent:\n",
    "        symptom_group.append(symptoms_to_boolean(recognize_symptoms_in_sentence(sentence, tokenized_sentences[i][j])))\n",
    "        j += 1\n",
    "    boolean_symptoms.append(symptom_group)\n",
    "    i += 1\n",
    "boolean_symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each group, merge all existing symptoms into one array\n",
    "def merge_symptom_info(symptom_arrays):\n",
    "    # Initialize null array\n",
    "    merged_symptoms = []\n",
    "    for symptom in symptom_list:\n",
    "        merged_symptoms.append(0)\n",
    "\n",
    "    for symptom_group in symptom_arrays:\n",
    "        for i, symptom in enumerate(symptom_group):\n",
    "            if symptom == 1:\n",
    "                merged_symptoms[i] = 1\n",
    "    return merged_symptoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge symptoms per respondent\n",
    "respondent_symptoms = []\n",
    "for respondent in boolean_symptoms:\n",
    "    respondent_symptoms.append(merge_symptom_info(respondent))\n",
    "respondent_symptoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([('hirap sa paghinga', 'pagsikip sa dibdib', 'ubo', ...)],\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "symptom_columns = []\n",
    "for symptom in symptom_list:\n",
    "    symptom_columns.append([symptom,],)\n",
    "\n",
    "cols = pd.MultiIndex.from_arrays(symptom_columns)\n",
    "print(cols)\n",
    "\n",
    "input_frame = pd.DataFrame(respondent_symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('naiveBayes.pkl', 'rb') as f:\n",
    "    naiveBayes = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['migraine', 'flu', 'migraine', 'migraine', 'migraine', 'migraine',\n",
       "       'migraine', 'migraine', 'altapresyon', 'toothache', 'dengue',\n",
       "       'migraine', 'migraine', 'migraine', 'migraine'], dtype='<U11')"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_frame = scaler.fit_transform(input_frame)\n",
    "\n",
    "prediction_results = naiveBayes.predict(input_frame)\n",
    "prediction_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED: Convert diseases to respective IDs for comparison\n",
    "def diseases_to_idx(list_of_disease_strings):\n",
    "    list_of_disease_idx = []\n",
    "    for disease in list_of_disease_strings:\n",
    "        for key, val in disease_list.items():\n",
    "            if (disease == key):\n",
    "                list_of_disease_idx.append(val)\n",
    "    return list_of_disease_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.371900826446281\n",
      "Recall: 0.45454545454545453\n",
      "F1-score: 0.37878787878787884\n",
      "[[[14  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[12  0]\n",
      "  [ 3  0]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 1  0]]\n",
      "\n",
      " [[13  0]\n",
      "  [ 2  0]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[ 4 10]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 1  0]]\n",
      "\n",
      " [[13  0]\n",
      "  [ 2  0]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 1  0]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix\n",
    "\n",
    "true_results = df_test.drop(['sentence'], axis=1)\n",
    "true_results = true_results.drop_duplicates()\n",
    "true_results = true_results['result']\n",
    "\n",
    "#y_pred = diseases_to_idx(prediction_results)\n",
    "#y_true = diseases_to_idx(true_results.values.tolist())\n",
    "\n",
    "print(\"Precision:\", precision_score(true_results,\n",
    "      prediction_results, average='macro', zero_division=0))\n",
    "print(\"Recall:\", recall_score(true_results, prediction_results, average='macro', zero_division=0))\n",
    "print(\"F1-score:\", f1_score(true_results,\n",
    "      prediction_results, average='macro', zero_division=0))\n",
    "\n",
    "print(multilabel_confusion_matrix(true_results, prediction_results))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
